# LLM Finetuning Experiments

This repo contains experiments with fine-tuning small language models (e.g., TinyLlama, Mistral) using Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA, QLoRA, AdapterFusion, etc.

## ðŸš€ Goals
- Test PEFT methods on small models
- Compare performance (loss, quality, speed, size)
- Explore custom datasets

## ðŸ“¦ Techniques Used
- [x] LoRA (via ðŸ¤— PEFT)
- [ ] QLoRA
- [ ] Prefix Tuning
- [ ] AdapterFusion

## ðŸ“‚ Structure
See the folder structure above.

## Models used:
- llama2-7B
- phi-3
- 
